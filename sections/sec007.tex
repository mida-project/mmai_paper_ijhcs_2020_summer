\section{Discussion}
\label{sec:discussion}

The optimal use of the {\it Assistant} within clinical workflows ({\bf RQ1}) remains to be determined.
The specificity advantage exhibited by the {\it Assistant} suggests that it could help to improve diagnostic accuracy and, therefore, unnecessary biopsies.
Our findings suggest that there is an evidence that the integration of the AI in the UI, can help for an earlier cancer detection. 
Moreover, as presented on the results section (Section~\ref{sec:results}), the introduction of AI was well received by clinicians, while our {\it Assistant} is above their expectations ({\bf RQ2}).
Finally, supported by our results (Section~\ref{sec:results}), we show that clinicians' satisfaction and acceptance ({\bf RQ3}) of AI assistance successfully impacted the intended aspects of expectations.
The next sections will describe and discuss this paper contributions in terms of clinical expectations and AI assistance.

\subsection{Contribution for Clinical Expectations}

Our work provides insights into feasible {\it AI-Assisted} mechanisms on medical imaging diagnosis.
In this study, clinicians agree that an {\it Assistant} could improve their workflow.
Actually, by accessing our usability measures ({\it i.e.}, SUS) we observe that 31 clinicians in 45 are open to adopt this new paradigm.
In terms of workload, the {\it Assistant} results are much better than the {\it Current} results.
In fact, only Effort was outperformed.
The diagnostic time performance was also improved for low, medium and high severities.
Although, the improvements for the medium severities are merely small improvements.
The rates of False-Negatives and False-Positives were also reduced.
Which means that we are decreasing the number of medical-errors.

If we address the provided feedback, we can see that 41 clinicians would like such as a system to enter their daily practice. Of those, 28 mentioned that the system will be part of an important asset, improving their job.
One important aspect of our approach was giving clinicians the opportunity to revise the AI recommendation.
At the same time, the AI recommendations are also providing the opportunity to {\it accept} or {\it reject} the suggested diagnosis.

In this study, we show that AI can be integrated on the clinical workflow ({\bf RQ1}), impacting the same diagnostic types.
Importantly, the proposed {\it Assistant} was integrated into a clinical radiology RR scenario.
The {\it Assistant} has significantly benefit clinicians on diagnostic time (Section~\ref{sec:results}).
Actually, 86\% of the clinicians are finding the {\it Assistant} is not complex.
Further, 84\% of the clinicians found that the system was not cumbersome.
Making the {\it Assistant} quick to interact with and trivial.
This will impact the workflow on a positive manner, in consideration of clinicians perceiving that the {\it Assistant} will bring less complexity to their workflow.
Indeed, we also verified that for both low and high severities, the {\it Assistant} improve the diagnostic time improving the final decision in more than 50 seconds per patient.
In addition, we are also improving the numbers of False-Positives and False-Negatives.
Therefore, the {\it Assistant} will impact the clinical workflow in terms of time and accuracy.

Our findings show that supporting explainability\footnotemark[20] and intelligibility\footnotemark[21]~\cite{Abdul:2018:TTE:3173574.3174156}, improved the acceptance and confidence of clinicians.
In fact, results are showing that about 37 clinicians are accepting our approach and are feeling confident using it.
A key factor to our results is pairing the {\it accept} or {\it reject} features with several visual explainability techniques (Figure \ref{fig:fig006}) that empower the clinicians' choice and sense of control.
Hence, the outcome is achieved for clinical expectations, answering the {\bf RQ2} question on how to improve (via interpretation of {\it heatmaps}) diagnostic interpretability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\footnotetext[20]{{\bf Explainability:} is the use of models that are able to summarize the reasons for Neural Network behaviour, gaining the user's trust, or producing insights about the decisions causes.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\footnotetext[21]{{\bf Intelligibility:} is defined by the use of inherently interpretable models or by developing methods for explaining otherwise overwhelmingly complex decisions.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We believe our {\it BreastScreening} framework can offer a substantial contribution to the breast cancer domain.
We show (Figures \ref{fig:fig007}, \ref{fig:fig008}, \ref{fig:fig009} and \ref{fig:fig010}) that user satisfaction and acceptance ({\bf RQ3}) can be improved, not only through even more accurate models, but also higher expectation adjustment techniques.
Such techniques, could also contribute to improve our UI explainability methods.
Not just using simple {\it heatmaps}, but also using other important image feature information.
This addresses an important gap in existing research on preparing clinicians for the introduction of {\it AI-Assistive} techniques of their workflow~\cite{Alkhatib:2019:SAT:3290605.3300760, challen2019artificial, shah2019artificial, szolovits2019artificial}.

With our results, we could verify the proposed research questions providing evidence of the design and integration of {\it AI-Assisted} methods on a medical imaging domain.
For the breast cancer diagnosis, the introduction of an {\it Assistant} could impact the clinical workflow ({\bf RQ1}) on a positive way.
By setting visual explainability techniques, we show how to support clinician expectations ({\bf RQ2}) of AI assistance and how to improve diagnostic interpretability.
Finally, we successfully measure the impact of expectation-setting intervention techniques ({\bf RQ3}) on satisfaction and acceptance of AI assistance in radiology.

\subsection{Contribution for AI Assistance}

While Human-AI interaction tools have traditionally been used to improve algorithms, we found that {\it AI-Assisted} mechanisms empowered clinicians in the medical imaging diagnosis.
More precisely, a medical {\it Assistant} can make itself more understandable to clinicians by providing some kind of explanation ({\it heatmaps}).

In this work, we assume a setting in which either the Human or AI (or both) has ground-truth knowledge of how to diagnose the patient and classify the breast severity (BIRADS).
With the {\it accept} and {\it reject} features, both Human and AI are learning together about the diagnostic task.
Such Human-AI interaction will improve clinicians' transparency as a result of this bidirectional process.
Nevertheless, our findings suggest new ways of improving AI transparency~\cite{Cai:2019:EEE:3301275.3302289}.

Quantitative results, such as usability ({\it i.e.}, SUS) or workload ({\it i.e.}, NASA-TLX), point to improvements in terms of satisfaction~\cite{Bonham:2019:ARS:3308557.3308726} and acceptance~\cite{Gambino:2019:DDR:3290607.3312916, Sonntag:2012:RMD:2166966.2167031}, as well as a positive {\it workflow} impact~\cite{DeBackere:2015:DPR:2826165.2826229}.
The magnitude of the SUS usability measure employed by clinicians to describe their experience with our {\it Assistant} showed ``good'' usability~\cite{info:doi/10.2196/18585} ({\it i.e.}, SUS $>$ 68) and improvements on the workload values.
We also achieve improvements in performance and accuracy.

From our results, we can point that the time performance was almost 2x faster with the {\it Assistant} setup and more than 2x accurate.
Apart from being passive recipients of the machine outputs, clinicians could play an active role, improving data for the learning process on a Human-AI collaboration.
Indeed, this interactive collaboration could help clinicians from mental models and increasing assistant transparency~\cite{amershi2014power, Cai:2019:HTC:3290605.3300234, Eslami:2016:FIL:2858036.2858494}.

Qualitative results, such as workshops, focus groups, affinity diagrams and feedback from the interviews, are making a strong contribution in terms of how decisions are rea\-ched across many clinician roles and contexts.
We received positive feedback regarding our {\it BreastScreening} assistant.
Specifically, the decision benefits from using an integrated AI for automatic breast classification on the UI.
Two main outcomes resulted from this qualitative study.
First of all, the adoption of {\it heatmaps}.
Second, what are the main requirements for clinicians, such as imaging modalities, the need for control ({\it i.e.}, the {\it accept} and {\it reject} features), and what volumes to choose.
With these two implemented outcomes, we could cover some of the hazards on our {\it Breast\-Screening} assistant prototype.

Since clinicians prefer to see the lesion with context, we need to provide color information regarding both the shape and size of the lesions (Figure \ref{fig:fig006}).
Therefore, we chose this technique to support the lesion visualization use case.
Also, the {\it AI-Assistant}, for a given patient, provides a BIRADS classification for each modality.
However, from the interviews, we realized it was paramount to provide a "global" classification of the exam, instead of modality based.
This means that our {\it AI-Assistant} must provide the worst-case classification as the global score.
Furthermore, the image-modality assigned to the highest BIRADS should be displayed in the UI, providing the "explainability" ({\it 6.2 Explain} button of Figure \ref{fig:fig005}) of such a global score.

In short, {\it BreastScreening} uses large amounts of data, due to its multi-view and multi-modality nature.
The very first step on both quantitative and qualitative studies was to extract from our study information a set of analysis - a design-thinking method, which was crucial to perform data collection and cluster information.
Specifically, data clustering resulted in the following clusters:
(i) MG (both CC and MLO views);
(ii) US; and
(iii) DCE-MRI volume.
Note that in (i) a large number of views are available, {\it e.g.}, ML, LM, LMO, late ML, among others.
Concerning (iii), radiologists acquire a large set of MRI volumes (T1, T2, T2 Fat-Sat, Diffusion, Dynamic Contrast-Enhanced (DCE), etc).
This initial study resulted in a consensus that allowed the selection of CC and MLO views, in the case of MG, and DCE-MRI in MRI.
This information was suppressed in the paper since our main focus was on the evaluation of the (quantitative and qualitative) impact of an AI integration, and where we assumed that the data collection was readily available.