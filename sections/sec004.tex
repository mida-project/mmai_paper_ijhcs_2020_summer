\section{BreastScreening}
\label{sec:system}
To validate the proposed design goals and research questions, we developed {\it BreastScreening}, a proof-of-concept fully functional prototype to be evaluated in a realistic clinical scenario. In the following subsections we describe the main features of the {\it BreastScreening} system.

\subsection{Implementation}

{\it BreastScreening-AI} was implemented (Figure~\ref{fig:fig003}) using {\it CornerstoneJS} \cite{urban2017lesiontracker} with a {\it NodeJS} server.
To feed the system, we selected image patient sets from the HFF clinical institution and uploaded the images into an {\it Orthanc} server~\cite{Jodogne2018}.
Three imaging modalities (MG, US and MRI) were provided for each patient.
The images were pre-processed and anonymized on the {\it Orthanc} server and then consumed by the {\it BreastScreening-AI} assistant.

This system is efficiently designed as a set of modules that can be reused in other imaging applications.
The {\it CornerstoneJS} family of libraries provide essential functions, such as i) image rendering; ii) DICOM retrieval; iii) tool support ({\em i.e.}, developed functionalities); and iv) interpretation.
To enable smooth drawing operations for manual labeling of the annotations, {\it CornerstoneJS} leverages canvas objects~\cite{mullie2019coreslicer}, which enables to accelerate the rendering process of dimensional graphics in the browser.

All image data used in this process are stored and retrieved from this {\it CornerstoneJS} library.
Moreover, {\it CornerstoneJS} is a web-based library with tool support for asynchronous execution, enabling the use of segmentation tools.
Lastly, the library permits the interpretation of images rather than the recording of the imaging findings.

The {\it BreastScreening} core was developed in {\it JavaScript} with {\it jQuery} for {\it HTML} document manipulation, event handling and {\it dicomParser} for parsing DICOM files.
The DICOM files can be loaded by drag-and-drop into the browser window on the {\it Orthanc} view.
Loaded images can be further displayed on both views, but with different visualization configurations.
After the loading stage, the images are automatically arranged according to the scan IDs from the DICOM files.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\centering
\includegraphics[width=0.975\columnwidth]{fig003}
\caption{{\it BreastScreening-AI} Architecture: the main components of the system are AI Application, Image Viewer, Datasets and DICOM Storage. The Image Viewer of {\it BreastScreening} framework will provide essential interaction tools for radiologists. A study list is fetched from the Orthanc server, and through CornerstoneJS the radiologist can manipulate the image, interacting with the assistant at the same time.}
\label{fig:fig003}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Once the set of medical images is loaded in the visualization viewport (Figure \ref{fig:fig005}) the user can interact with the data by manipulating the visualization through the mouse and keyboard.
For instance, rolling the mouse wheel to navigate across the volume slices (MRI) or even moving the mouse to drag-and-drop the several modalities to the viewport.
The goal of our multi-modal strategy is then to provide the visualization and manipulation of several modalities of image to compare lesion patterns among those images.
This requires a novel mechanism for visualizing the three modalities of medical images (MG, US and MRI).

\subsection{User Interface}

Based on identified user needs (Section~\ref{sec:environment}), we designed and implemented the UI for {\it BreastScreening}.
The UI includes a set of refinement mechanisms to guide clinicians during the diagnostic process.
The UI of {\it BreastScreening} consists of two main components, comprising the list of patients (Figure \ref{fig:fig004}) and medical imaging views (Figure \ref{fig:fig005}).
Figure \ref{fig:fig004} can be framed in the following subcategories:
{\it 4.1. List of Patients};
{\it 4.2. Header};
{\it 4.3. Help};
{\it 4.4. About}; and
{\it 4.5. Study List Tabs}.
Figure \ref{fig:fig005} can also be subdivided in the following categories:
{\it 5.1. Viewports};
{\it 5.2. Toolbars}; and
{\it 5.3. Modality Selection}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hfill
\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig004}
\caption{{\it BreastScreening} provides clinician support to easily access and switch between the patient views. When accessing the assistant, the first screen shows the list of patients. Each row of the table, represents a patient. Each patient has a {\bf Patient ID}, {\bf Study Date}, {\bf Modality} (one or more), {\bf Study Description} and number of {\bf Images}.}
\label{fig:fig004}
\end{figure}
\hfill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The first assistant view is the {\it 4. List of Patients Views}.
With this view, clinicians can quickly choose the respective {\it 4.1. List of Patients}.
Using the BI-RADS functionality ({\it 6.3.1.~Physician Severity}), clinicians can now classify (Figure \ref{fig:fig005}) the severity of the breast lesion for each patient.
This {\it 4.1. List of Patients} contains only the most important and required information, avoiding the excess of data ({\it e.g.}, gender, access activity on the file, etc), typically shown on these systems.
Therefore, improving the medical imaging ({\it MID}) visualization across the radiology room.
The {\it 4.5. Study List Tabs} also allows clinicians to switch between diagnosed patients, {\it 5. Medical Imaging Diagnosis Views}, and {\it 4. List of Patients Views}.

Upon starting the UI in the web browser, the user can select from the {\it 4.1. List of Patients} available in the study list.
Hence, it will improve the design around medical imaging ({\it MID}) at a higher diagnostic accuracy level (Section~\ref{sec:results}) from a single clinician.
From here, users can search all patients using criteria such as {\bf Patient ID}, {\bf Study Date}, available {\bf Modality} sets, {\bf Study Description}, and number of {\bf Images}.
By pressing in a selected row, clinicians open the image viewers.
In the image viewers, it is possible to visualize the image modalities.

To support clinicians, we provide both {\it 4.3. Help} and {\it 4.4. About} components.
The components are providing clinicians information about how to classify and information about the system, as well as contact information.
For instance, these components were used by Interns, to better understand how they could use the tool for diagnosis.

The {\it 5.2. Toolbars} (Figure \ref{fig:fig005}) were positioned to match the users's requirements (conclusions drawn during our observations and interviews).
This corresponds to the MID goal.
The {\it 5.1. Viewports} are displayed at the right, after the {\it 5.2. Toolbars}.

Clinicians can probe for lesion patterns via the {\it 5.1.~Viewports} and process the image by using the {\it 5.2.~Toolbars} features ({\it MID}).
Each time the {\it 5.2.~Toolbars} functionalities are activated, the clinician needs to perform a simple and easy interaction with the medical image to manipulate it as desired.
Using the {\it 5.2.~Toolbars} on the {\it 5.1.~Viewports}, the clinician can locate the lesions and classify its severity (BI-RADS).

\subsubsection{Assistant}

The {\it 6.1.~Accept} or {\it 6.3.~Reject} allows the clinician to accept (or reject) the automatic classification of the {\it 6.~Assistant}.
The {\it 6.~Assistant} is based on the use of a DenseNet~\cite{huang2017densely}, following several steps (Section~\ref{app:steps} of Appendix~\ref{app:app001}) of the training set.
However, integrating a deep neural network (DNN) needs special attention.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\columnwidth]{fig005}
\caption{Our {\it BreastScreening-AI} assistant provides several features regarding the basics of {\it Radiomics}. From there, we will be able to validate our DenseNet BI-RADS classifier along with clinicians.}
\label{fig:fig005}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Typically, the training of a DNN is expensive regarding the time spent, since its classification performance will improve as the training dataset becomes larger.
Thus, training the assistant from the scratch is not the best option.
For this reason, we pre-train ({\it i.e.} off-line training before the UI integration) the DNN on ImageNet~\cite{deng2009imagenet} dataset and fine-tuned using our multi-modal breast {\it dataset}.
Specifically, we fine-tune the DNN using supervised learning.
Only afterwards, the pre-trained DenseNet is incorporated into the UI.

Our DenseNet takes medical images as an input and outputs the severity probability.
The input consists of images ({\it i.e.} MG, US and MRI slices) with the corresponding label ({\it i.e.}, the BI-RADS) that is previously classified by an expert.
The output consists of having five nodes in the last layer of the DNN.
Each node is assigned to a given class that corresponds to each BI-RADS score.
After this stage, we have conditions for the integration, since now the DenseNet is tuned to perform the classification in unseen test images.
The classification is fast, being tailored for an online diagnosis.
When several modalities ({\it MID} and {\it CRD}) are correctly used (regarding the {\it 5.3.~Modality~Selection} on a {\it Multimodality} view), the clinician can find more accurately the right severity classification, as concluded in Section~\ref{sec:results}.
For the {\it 6.3.~Reject} option, the physician will have the opportunity to insert ({\it CRD}) the proposed BI-RADS on a drag-and-drop menu ({\it 6.3.1.~Physician Severity}) of severity options.
Both {\it MID} and {\it CRD} goals are supporting our {\bf RQ1} question.

\subsubsection{Explainability}

Finally, the clinician may look for the {\it 6.2.~Explain} feature  by looking at the generated heatmaps (Figure~\ref{fig:fig006}).
The generation of the above maps come from the following information: (i) the area ({\it Lesion Size}) of the lesion that comes from the delineation process, (ii) the circularity/sharpness ({\it Lesion Shape}) that can be computed from the annotation in (i), and (iii) the BI-RADS score automatically provided by the DenseNet ({\it i.e.}, AI assistant).
The {\it ED} design goal is performed as above described helping to answer to research questions {\bf RQ2} and {\bf RQ3}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig006}
\caption{On {\it 6.2.~Explain}, the assistant will pop-up several {\it heatmaps}. The {\it heatmaps} represent two different scales: (a) {\it Shape}; and (b) {\it Size} of the lesion. First, the model computes the BI-RADS using the DenseNet classification. Second, the model computes the lesion circularity (top scale) and size (bottom scale) and associate it to the colors regarding both circularity/size and BI-RADS.}
\label{fig:fig006}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%