\section{Conclusion}
\label{sec:conclusion}

Clinical translation of {\it radiomics} is a promising but challenging research topic.
The integration of AI techniques into the clinical workflow requires a holistic approach which can benefit from an HCI perspective such as the one provided here.
In our work, we need to consider several factors when making decisions about how to add AI to medical practice.
For instance, bias is a familiar concept for clinicians, who are already trained to practice evidence-based medicine.
On the other hand, AI can suffer from selection bias ({\it i.e.}, of training {\it datasets}), automation bias, and data shift.
We must carefully balance trust and risk in AI implementation, by acknowledging remarks and limitations, as well as potential consequences of AI-driven diagnosis.
Our {\it Assistant} design concerns the interaction of clinicians and AI in a real-world setting.
Further work is necessary to transition AI from highly controlled experimental environments to real life practice.

\subsection{Main Contributions}

In this paper, we implemented and studied the introduction of an assistant in the clinical workflow as a tool to classify and explain medical imaging diagnosis in the breast cancer domain.
Indeed, the results are showing that AI can positively impact the clinical workflow at a cost of providing clinician control over the proposed BI-RADS classification.
Specifically, medical assistants are contributing to an increase of clinicians' perception of trust.
The proposed investigations focus on two different contexts.
As follows, each of the two different contexts will be detailed.

First of all, we applied these developments as a second reader assistant mimiking the expert domain of breast cancer interpretation and classification for a second opinion.
{\it BreastScreening-AI} was developed on the top of {\it BreastScreening} core to study the impact of AI-assistance on the medical imaging workflow as an autonomous and imidiatly available second reader.
Insights from an experiment with 45 clinicians showed that a medical assistant can make itself more understandable by providing some type of explanation ({\it i.e.}, {\it heatmaps}) without comprising its reliability.
Moreover, satisfaction and acceptance were significantly improved by the assistant with this strategy.

Second, a study was promoted to understand how AI assistants are (positively) affecting the medical workflow.
Clinicians' accuracy was compared with and without AI, while studying the impact of several design techniques in terms of clinicians' expectations and satisfaction.
Consistent with expectations, the results are showing clinicians' accuracy and acceptance of an AI system optimized for high specificity can be significant higher than a system optimized for higher sensitivity values.
Furthermore, by measuring FP/FN and relating to time performance, results are suggesting that the proposed adjustment techniques successfully impact the intended aspects of expectations.
Finally, the results are showing that these techniques are successful in reducing variability among the clinical groups of experts on an AI system.
Thus, converging to a more reliable final diagnostic by reducing the independence levels of less experient clinicians ({\it i.e.}, Interns and Juniors) in disagreement~\cite{schaekermann2018expert} with higher expert clinicians' ({\it i.e.}, Middles and Seniors) results.

To conclude, it was demonstrated how an AI system focus on the communication of model performance ({\it e.g.}, specificity and sensitivity) can lead to much higher perceptions of accuracy.
Due to a model performance communication, intelligent agents are increasing the acceptance of medical professionals by showing the visual representations for classification, performance and segmentation of lesions.
With these visual representations, model ambiguity~\cite{SchaekermannMike2020} can be adjudicated~\cite{10.1145/3359178, 10.1167/tvst.8.6.40} by clinicians to control the results of the final diagnostic.
Therefore, clinicians are increasing their AI confidence with higher values of {\it trust}.

\subsection{Findings}

Medical AI-assistants are representing a type of system that is passive ({\it i.e.}, assistive as a second reader) and casual.
Clinicians can {\it accept}, {\it reject} and ask the system to {\it explain}, or even {\it ignore} the assistant result.
While this is only one class of CDSS, we believe that this class represents many current efforts of integrating HAII into other~\cite{Savage2019, shah2019artificial, topol2019high} clinical domains ({\it e.g.}, systems for clinical drug development, epidemiology, dementia treatment, etc).

\hfill

\noindent
Findings should be generalized to other clinical systems and tasks according to the following claims:

\begin{enumerate}
\item Evaluations are task agnostic as they are informed by the high-level workflow actions and autonomous diagnostic mechanisms in which clinicians make decisions, while a workflow understanding is crucial to recognize clinicians' needs;
\item Intelligent agents should provide concrete empirical evidence and insights by detailing the benefits of avoiding the different types of AI errors and diagnostic mistakes to address model ambiguity in various steps of the AI pipeline;
\item An AI system based on {\it Precision} and {\it Recall} optimization will improve higher values of FPs and FNs as model uncertainty and explanations are provided to clinicians so that they can be aware of model misunderstandings;
\end{enumerate}

The importance of avoiding different types of errors depends on the information available from a specific clinical domain.
Indeed, this is a complex issue.
While it could be argued that for the breast cancer diagnosis FNs are always better to avoid, such results can not always suggest that they are independently to the clinical domain.
In this thesis, avoiding FNs might be better.
Actually, avoiding FNs can be better as a result of escaping from saying there is no cancer, when surely there is.
However, in other clinical domains an FP might be more important to avoid.
For instance, in drug development, the domain wants to prefer avoiding the FP rates.
As a consequence, the applications of AI systems are optimizing the number of chemicals and drugs identified incorrectly by the system~\cite{raja2017machine}.

We believe that the achieved main findings are generalized to a clinical class of passive systems ({\it i.e.}, clinicians are making the final decisions) in which the end ratios of workload regarding both FPs and FNs are lower.
In critical systems, it is more important to analyze the severity of consequences.
However, the severity of consequences shall be applied to different clinical errors rather than medical imaging workload improvements.

\subsection{Design Implications}

The main findings have implications for different stages of the design of an intelligent agent for CDSS solutions.
From data collection across model training to the design of UI for AI systems, the implications of design are influencing the final proposed solution of this work.
In this section, we describe several design implications and recommendations applied until now, as well as the ones that will be applied in the future.

\subsubsection{Data Collection}

Developing an AI system for providing explanations across patient cases would require that structured information is given in the training data.
On the one hand, several approaches are addressing the problem of collecting unstructured medical data~\cite{10.1145/3308560.3317085, SchaekermannMike2020} with open-ended arguments for classification cases.
On the other hand, recent works are demonstrating that imposing structure in the data collection process can facilitate a deeper understanding of clinician disagreement with AI~\cite{10.1145/3308560.3317085} and accelerate consensus formation~\cite{10.1145/3313831.3376506}.
Thus, a data collection procedure is recommended for AI-based systems by being equipped with structured procedures to benefit from these findings and facilitate the development of AI-assistance for medical imaging.

\subsubsection{Model Training}

This study suggests that medical workflows and trust can be positively affected by the introduction of intelligent agents which are endowing AI-based CDSS with the ability to not only make BI-RADS classification suggestions, but also to identify potential lesions.
Implementation of such AI systems would require that supervised ML models are equipped with additional prediction targets ({\it e.g.}, automatic segmentation and classification of patient co-variables) beyond severity classification (BI-RADS) alone.
These additional targets could include automatic classification of breast density, while warning clinicians the model accuracy for being in the presence of these cases.
These another additional target is showing information concerning the weights of each co-variable for the achieved classification.
Additional targets could be integrated either into one joint training process or by developing several separate models, one for each target.
Mehta et al.~\cite{10.1007/978-3-030-00934-2_99} describes how to bring a multi-target approach into the classification and segmentation of medical images in one joint training process.
These design implications were brought from the focus groups, but were not yet addressed under this thesis proposal.
However, they will be addressed as future work.

\subsubsection{User Interface Considerations}

In this paper, several ways of displaying and explaining breast cancer diagnosis were evaluated by visually showing explainable (XAI).
These explainable (XAI) techniques are coming from the trained models and are informing clinicians in several ways.
While the achieved results may suggest that these representations should be effective, it is recommended further future work.

Further directions could explore more complex techniques for proper information visualization and design considerations.
For instance, it is important to understand if heatmaps are effective and final ways for the representation of lesion contours with respect (colors) to the severity classification, or if there are other techniques ({\it e.g.}, bounding-boxes) to better inform clinicians.
Moreover, it could be important to study whereas the design for information visualization of model performance would support a better patient diagnostic.

UI design considerations are facilitating and constraining the communication between human users and AI models (HAII).
The design should be carefully decided whether and how the information is exposed to the end-users, so that the end-users can strike the right balance for a trusted, efficient and effective interaction.
Clinicians' trust can be promoted by highlighting and explaining instances ({\it i.e.}, co-variables) of important patient information as predicted by an AI model.
However, factors like domain-specific tolerance may affect whether exposing wrong patient information contributes to the establishment or erosion of clinicians' trust.

A predicted likelihood of showing AI accuracy for a given case can be a useful criterion for clinicians.
Actually, by choosing which AI-suggestion to review first, the UI design is saving clinicians' time and cognitive resources ({\it e.g.}, not showing heatmaps immediately) while diagnosing each patient.
In fact, the given model explanations should be human-interpretable, case-specific, and accurate.

An inaccurate or irrelevant explanation may significantly harm a clinician's ability to diagnose a patient case correctly.
As a consideration, if accurate explanations cannot be reliably produced, they should not be exposed to the end-user.
Hence, low accuracy classification must be actively warned to the clinician about the inaccurate final result.

Impact of explanation is a factor that should determine whether clinicians are exposed to explainable (XAI) information.
For an efficient interaction, it is recommended that clinicians should not be exposed to granular explainable information for patient cases where accuracy interpretation cannot yield straight decision-making outcomes.
As an example, if the model accuracy is low for a specific co-variable classification, the assistant should omit that co-variable, since it was not properly classified.
Thus, it will be of chief importance to study at what level of accuracy are clinicians accepting each co-variable to be less accurately classified.

\subsubsection{Medical Imaging Perspectives}

Due to its multi-view and multi-modal nature, the assistant uses large amounts of data to inform clinicians.
The very first step in both quantitative and qualitative studies was to extract from our study information a set of analysis~\textendash~a human-centered design methodology~\textendash~which was crucial to answer our design choices.
Specifically, data clustering resulted in the following clusters:
(i) MG images (both CC and MLO views);
(ii) US images; and
(iii) MRI volumes.
Note that in (i) a large number of views are available, {\it e.g.}, ML, LM, LMO, late ML, among others.
Concerning (iii), radiologists acquire a large set of MRI ({\it e.g.}, T1, T2, Diffusion, DCE) volumes~\cite{seifabadi2019correlation}.
This initial design choices resulted in a consensus that allowed the selection of CC and MLO views, in the case of MG images, and MRI volumes.

\subsection{Limitations}

Unfortunately, due to time constrains of clinicians, it was not possible to investigate the impact of other AI-assistance techniques.
Furthermore, the assistant represents a type of system that is passive and casual~\cite{Kocielnik:2019:YAI:3290605.3300641}, where the impact of AI imperfections in the clinical workflow is arguably critical.
Breast cancer screening may only be effective if clinicians accept the new diagnostic paradigm with the introduction of useful, yet imperfect, AI techniques.
Despite of this limitation, the achieved results are indicating that the proposed assistant may successfully introduce behavioral changes into the clinical workflow.

Another important point to follow are the limitations of DL systems.
These methods are typically seen as black-boxes \cite{litjens2017survey} and, therefore, difficult to ``explain'' without more varied data~\cite{10.1145/3313831.3376290}.
In this work, it was only addressed the BI-RADS classification problem and {\it heatmap} segmentation features.
However, clinical co-variables are also important to better inform and explain to clinicians the DL results.
Additionally, it is important to understand the best communication strategies to inform clinicians about the importance of each co-variable in the final DL result.
Thus, such concerns are addressed in the future work.

\subsection{Future Work}

As a future work, the paper proposes the application of a model based on the UTAUT~\cite{venkatesh2016unified} to study the adoption of AI systems in medical imaging diagnosis.
The idea is to test the model via confirmatory factor analysis and structural equation modeling while using clinicians' responses to a formulated UTAUT questionnaire.
Findings will provide valuable contributions to HCI and AI researchers concerning the design and implementation of intelligent agents.
Future directions will analyse the critical behaviour and implement persuasive mechanisms to reduce the rates of FPs and FNs.
The goal will be to understand how the level of assertiveness~\cite{pacheco2019alignment, 10.1145/3311350.3347162} will impact the clinicians' decision-making process during breast cancer diagnosis.
A proof-of-concept prototype will be developed with two major scenarios of assistant behaviour:
(1) Assertive; and
(2) Non-Assertive;
paired with two assistant behaviours:
(i) Proactive; and
(ii) Reactive.

Future developments must follow the literature implementation of explainable (XAI) and interpretability methods suggested by research works~\cite{9233366}.
The various methods will show to clinicians different dimensions in interpretability research, from providing ``obviously'' explainable and interpretable information on complex patterns of lesions and patient co-variables.
However, it will be also important to evaluate the quality of explanations and levels of interpretability given by the introduction of these new XAI methods.
Therefore, the System Causability Scale (SCS)~\cite{andreas2020measuring} will support future studies of this thesis.
Image data availability is an important hurdle for the implementation of AI in the clinical setting.
Thus, a dataset will be published including not only clinical data, but also user results.
Clinical data will be published containing important information to the ML algorithms, such as medical images, classifications and segmentations.
Finally, user results will be also published containing usability (SUS) and workload (NASA-TLX) measures, as well as trust and causability (SCS), between other future metrics to be applied.